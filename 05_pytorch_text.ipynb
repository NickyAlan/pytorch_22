{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import torchdata\n",
    "from torchtext.datasets import AG_NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(AG_NEWS(split='train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 : World\n",
    "2 : Sports\n",
    "3 : Business\n",
    "4 : Sci/Tec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "def yield_tokens(data_iter) :\n",
    "    for _, text in data_iter :\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>']) # Unknown\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[335, 282, 423, 62, 1199, 1164, 0, 0]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['man','i','do','not','know','why','promethazine','acetaldehyde'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95811"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. \n",
    "- The label pipeline converts the label into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x : vocab(tokenizer(x))\n",
    "label_pipeline = lambda x : int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[335, 282, 423, 62, 1199, 1164, 0, 0]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('man i do not know why promethazine actaldehyde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data batch and iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch) :\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch :\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module) :\n",
    "    def __init__(self, vocab_size, embed_dim, num_class) :\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.dense = nn.Linear(in_features = embed_dim, out_features = num_class)\n",
    "        self.init_weight()\n",
    "    \n",
    "    def init_weight(self) :\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.dense.weight.data.uniform_(-initrange, initrange)\n",
    "        self.dense.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets) :\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.dense(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(set([label for label,text in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "modelV1 = TextClassificationModel(vocab_size, emsize, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassificationModel(\n",
       "  (embedding): EmbeddingBag(95811, 64, mode=mean)\n",
       "  (dense): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_step(model, dataloader, optimizer, loss_fn) :\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "\n",
    "    for batch, (label, text, offsets) in enumerate(dataloader) :\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = loss_fn(predicted_label, label)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_acc += (torch.argmax(torch.softmax(predicted_label, dim=1), dim=1) == label).sum().item() / len(predicted_label)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    total_acc = total_acc / len(dataloader)\n",
    "    total_loss = total_loss / len(dataloader)\n",
    "    return total_acc, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, dataloader, loss_fn) :\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "\n",
    "    with torch.inference_mode() :\n",
    "        for batch, (label, text, offsets) in enumerate(dataloader) :\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = loss_fn(predicted_label, label)\n",
    "            total_acc += (torch.argmax(torch.softmax(predicted_label, dim=1), dim=1) == label).sum().item() / len(predicted_label)\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "    total_acc = total_acc / len(dataloader)\n",
    "    total_loss = total_loss / len(dataloader)\n",
    "    return total_acc, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataset.Subset at 0x1c72ba0a140>,\n",
       " <torch.utils.data.dataset.Subset at 0x1c72ba09660>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "lr = 0.1\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(modelV1.parameters(), lr=lr)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.9)\n",
    "split_train, split_val = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "split_train, split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3375, 375, 238)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(split_train, BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(split_val, BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "len(train_dataloader), len(val_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch) :\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch :\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1111,  4276,   465, 14198,    24,   903,     4,   372,  3906,   783,\n",
      "          864,     3,  7005,     1,    15, 21292,  1862,   677,    62,   189,\n",
      "          442,     4, 13558,    32,    78,     1,   108,   960,     6,  1322,\n",
      "          783,   864,     1,    45,     2,   335,    75,   747,   283,   423,\n",
      "           25,  2328,    12,    83,   435, 18624,     4,  3736,     1]) \n",
      " torch.Size([49]) \n",
      " 49\n",
      "tensor([21633,    84,  3881,    33,   203,   379,    30,   365,   388,  2253,\n",
      "        25589, 21633, 18953,     3,   141,   199,   125,     8,    76,     6,\n",
      "            2,   379,  1007,   176,     3,    10,    60,    26, 81390,    17,\n",
      "           34,  1872,     2,  6465,   199,   125,  4991,  3881,    95,   939,\n",
      "          379,    66,     5,  3799,   365,   388,     1]) \n",
      " torch.Size([47]) \n",
      " 47\n",
      "tensor([  630,   810,  5938,    11, 10489,     4,   149,   614,  4734,    10,\n",
      "            2,  1091,    29,  7310,     4,   149,   614,   107,   192,    12,\n",
      "           83,    37,  2708,    10,    30,  5622,     1,  1093,   149,   614,\n",
      "        15371,    42,   235, 21795,   839,    17,   214]) \n",
      " torch.Size([37]) \n",
      " 37\n",
      "tensor([  197,   176,  2717,   554,  2550,    11,   605,   574,   608,    13,\n",
      "           27,    14,    15,   197,   176,   554,    28,  1052,     2,  3641,\n",
      "            6,     5,   184,  1466,   736,  2550, 10501,     4,   403,    16,\n",
      "            9,  3093, 22799,     3,    45,  6489,   221,   104,    37, 14848,\n",
      "            4,     2, 36499,    11,     2,   200,    16,     9,    47,    71,\n",
      "            3,     5,   131,   176,   293,    26,    10,    65,     1]) \n",
      " torch.Size([59]) \n",
      " 59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 49, 47, 37, 59]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 0\n",
    "_offsets = [0]\n",
    "for _label, _text in split_train :\n",
    "    a = torch.tensor(text_pipeline(_text))\n",
    "    print(a ,'\\n', a.shape,'\\n', a.size(0))\n",
    "    r+=1\n",
    "    _offsets.append(a.size(0))\n",
    "    if r == 4 :\n",
    "        break\n",
    "_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,  49,  96, 133])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(_offsets[:-1]).cumsum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train accuracy 0.411 | valid accuracy 0.516 \n",
      "-----------------------------------------------------------\n",
      "Epoch 2 | train accuracy 0.571 | valid accuracy 0.620 \n",
      "-----------------------------------------------------------\n",
      "Epoch 3 | train accuracy 0.664 | valid accuracy 0.705 \n",
      "-----------------------------------------------------------\n",
      "Epoch 4 | train accuracy 0.728 | valid accuracy 0.758 \n",
      "-----------------------------------------------------------\n",
      "Epoch 5 | train accuracy 0.769 | valid accuracy 0.788 \n",
      "-----------------------------------------------------------\n",
      "Epoch 6 | train accuracy 0.795 | valid accuracy 0.808 \n",
      "-----------------------------------------------------------\n",
      "Epoch 7 | train accuracy 0.813 | valid accuracy 0.820 \n",
      "-----------------------------------------------------------\n",
      "Epoch 8 | train accuracy 0.827 | valid accuracy 0.829 \n",
      "-----------------------------------------------------------\n",
      "Epoch 9 | train accuracy 0.838 | valid accuracy 0.836 \n",
      "-----------------------------------------------------------\n",
      "Epoch 10 | train accuracy 0.846 | valid accuracy 0.841 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS+1) :\n",
    "    train_acc, train_loss = train_step(modelV1, train_dataloader, optimizer, loss_fn)\n",
    "    val_acc, val_loss = test_step(modelV1, val_dataloader, loss_fn)\n",
    "\n",
    "    print(f'Epoch {epoch} | train accuracy {train_acc:.3f} | valid accuracy {val_acc:.3f} ')\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | train accuracy 0.853 | valid accuracy 0.847 \n",
      "-----------------------------------------------------------\n",
      "Epoch 12 | train accuracy 0.858 | valid accuracy 0.853 \n",
      "-----------------------------------------------------------\n",
      "Epoch 13 | train accuracy 0.863 | valid accuracy 0.857 \n",
      "-----------------------------------------------------------\n",
      "Epoch 14 | train accuracy 0.868 | valid accuracy 0.862 \n",
      "-----------------------------------------------------------\n",
      "Epoch 15 | train accuracy 0.871 | valid accuracy 0.864 \n",
      "-----------------------------------------------------------\n",
      "Epoch 16 | train accuracy 0.875 | valid accuracy 0.867 \n",
      "-----------------------------------------------------------\n",
      "Epoch 17 | train accuracy 0.878 | valid accuracy 0.869 \n",
      "-----------------------------------------------------------\n",
      "Epoch 18 | train accuracy 0.880 | valid accuracy 0.871 \n",
      "-----------------------------------------------------------\n",
      "Epoch 19 | train accuracy 0.883 | valid accuracy 0.874 \n",
      "-----------------------------------------------------------\n",
      "Epoch 20 | train accuracy 0.885 | valid accuracy 0.877 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(11, EPOCHS+11) :\n",
    "    train_acc, train_loss = train_step(modelV1, train_dataloader, optimizer, loss_fn)\n",
    "    val_acc, val_loss = test_step(modelV1, val_dataloader, loss_fn)\n",
    "\n",
    "    print(f'Epoch {epoch} | train accuracy {train_acc:.3f} | valid accuracy {val_acc:.3f} ')\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassificationModel(\n",
       "  (embedding): EmbeddingBag(95811, 64, mode=mean)\n",
       "  (dense): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(modelV1.state_dict(), './save_model/modelV1_nlp1.pth')\n",
    "modelV1_loaded = TextClassificationModel(vocab_size, emsize, num_class)\n",
    "modelV1_loaded.load_state_dict(torch.load(f='./save_model/modelV1_nlp1.pth'))\n",
    "modelV1_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([95811, 64])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelV1_loaded.state_dict()['embedding.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8696165966386554, 0.39370154457933765)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, test_loss = test_step(modelV1_loaded, test_dataloader, loss_fn)\n",
    "test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "AG_NEWS_label = ['World', 'Sports', 'Business', 'Sci/Tec']\n",
    "\n",
    "def predict(model, text, text_pipeline) :\n",
    "    model.eval()\n",
    "    with torch.inference_mode() :\n",
    "        text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        pred = model(text, torch.tensor([0]))\n",
    "        probas = torch.softmax(pred, dim=1)\n",
    "        pred_class = torch.argmax(probas, dim=1)\n",
    "\n",
    "    return probas[0], AG_NEWS_label[pred_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_text_str = \"MEMPHIS, Tenn. - Four days ago, Jon Rahm was \\\n",
    "    enduring the seasons worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursdays first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering hed never played the \\\n",
    "    front nine at TPC Southwind.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0403, 0.9428, 0.0064, 0.0105]), 'Sports')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas, pred_class = predict(modelV1_loaded, ex_text_str, text_pipeline)\n",
    "probas, pred_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63e79917a05e390872358bfb73c58bc903ada01d2d04077091749088207d82cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
