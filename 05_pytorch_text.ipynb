{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import torchdata\n",
    "from torchtext.datasets import AG_NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(AG_NEWS(split='train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 : World\n",
    "2 : Sports\n",
    "3 : Business\n",
    "4 : Sci/Tec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "def yield_tokens(data_iter) :\n",
    "    for _, text in data_iter :\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>']) # Unknown\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[335, 282, 423, 62, 1199, 1164, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['man','i','do','not','know','why','promethazine','acetaldehyde'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95811"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. \n",
    "- The label pipeline converts the label into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x : vocab(tokenizer(x))\n",
    "label_pipeline = lambda x : int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[335, 282, 423, 62, 1199, 1164, 0, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('man i do not know why promethazine actaldehyde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data batch and iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch) :\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch :\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module) :\n",
    "    def __init__(self, vocab_size, embed_dim, num_class) :\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.dense = nn.Linear(in_features = embed_dim, out_features = num_class)\n",
    "        self.init_weight()\n",
    "    \n",
    "    def init_weight(self) :\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.dense.weight.data.uniform_(-initrange, initrange)\n",
    "        self.dense.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets) :\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.dense(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(set([label for label,text in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "modelV1 = TextClassificationModel(vocab_size, emsize, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassificationModel(\n",
       "  (embedding): EmbeddingBag(95811, 64, mode=mean)\n",
       "  (dense): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_step(model, dataloader, optimizer, loss_fn) :\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "\n",
    "    for batch, (label, text, offsets) in enumerate(dataloader) :\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = loss_fn(predicted_label, label)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_acc += (torch.argmax(torch.softmax(predicted_label, dim=1), dim=1) == label).sum().item() / len(predicted_label)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    total_acc = total_acc / len(dataloader)\n",
    "    total_loss = total_loss / len(dataloader)\n",
    "    return total_acc, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, dataloader, loss_fn) :\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "\n",
    "    with torch.inference_mode() :\n",
    "        for batch, (label, text, offsets) in enumerate(dataloader) :\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = loss_fn(predicted_label, label)\n",
    "            total_acc += (torch.argmax(torch.softmax(predicted_label, dim=1), dim=1) == label).sum().item() / len(predicted_label)\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "    total_acc = total_acc / len(dataloader)\n",
    "    total_loss = total_loss / len(dataloader)\n",
    "    return total_acc, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataset.Subset at 0x24722099ed0>,\n",
       " <torch.utils.data.dataset.Subset at 0x2472209ba60>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "lr = 0.1\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(modelV1.parameters(), lr=lr)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.9)\n",
    "split_train, split_val = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "split_train, split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3375, 375, 238)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(split_train, BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(split_val, BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "len(train_dataloader), len(val_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Epoch 1 | train accuracy 0.8099 | valid accuracy 0.8081 \n",
      "-----------------------------------------------------------\n",
      "Epoch 2 | train accuracy 0.8239 | valid accuracy 0.8197 \n",
      "-----------------------------------------------------------\n",
      "Epoch 3 | train accuracy 0.8342 | valid accuracy 0.8298 \n",
      "-----------------------------------------------------------\n",
      "Epoch 4 | train accuracy 0.8424 | valid accuracy 0.8358 \n",
      "-----------------------------------------------------------\n",
      "Epoch 5 | train accuracy 0.8496 | valid accuracy 0.8433 \n",
      "-----------------------------------------------------------\n",
      "Epoch 6 | train accuracy 0.8549 | valid accuracy 0.8484 \n",
      "-----------------------------------------------------------\n",
      "Epoch 7 | train accuracy 0.8599 | valid accuracy 0.8528 \n",
      "-----------------------------------------------------------\n",
      "Epoch 8 | train accuracy 0.8645 | valid accuracy 0.8567 \n",
      "-----------------------------------------------------------\n",
      "Epoch 9 | train accuracy 0.8683 | valid accuracy 0.8608 \n",
      "-----------------------------------------------------------\n",
      "Epoch 10 | train accuracy 0.8711 | valid accuracy 0.8641 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS+1) :\n",
    "    train_acc, train_loss = train_step(modelV1, train_dataloader, optimizer, loss_fn)\n",
    "    val_acc, val_loss = test_step(modelV1, val_dataloader, loss_fn)\n",
    "\n",
    "    print(f'Epoch {epoch} | train accuracy {train_acc:.3f} | valid accuracy {val_acc:.3f} ')\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassificationModel(\n",
       "  (embedding): EmbeddingBag(95811, 64, mode=mean)\n",
       "  (dense): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(modelV1.state_dict(), './save_model/modelV1_nlp1.pth')\n",
    "modelV1_loaded = TextClassificationModel(vocab_size, emsize, num_class)\n",
    "modelV1_loaded.load_state_dict(torch.load(f='./save_model/modelV1_nlp1.pth'))\n",
    "modelV1_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8618697478991597, 0.42128313232620224)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, test_loss = test_step(modelV1_loaded, test_dataloader, loss_fn)\n",
    "test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "AG_NEWS_label = ['World', 'Sports', 'Business', 'Sci/Tec']\n",
    "\n",
    "def predict(model, text, text_pipeline) :\n",
    "    model.eval()\n",
    "    with torch.inference_mode() :\n",
    "        text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        pred = model(text, torch.tensor([0]))\n",
    "        probas = torch.softmax(pred, dim=1)\n",
    "        pred_class = torch.argmax(probas, dim=1)\n",
    "\n",
    "    return probas[0], AG_NEWS_label[pred_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_text_str = \"MEMPHIS, Tenn. - Four days ago, Jon Rahm was \\\n",
    "    enduring the seasons worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursdays first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering hed never played the \\\n",
    "    front nine at TPC Southwind.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0403, 0.9356, 0.0130, 0.0111]), 'Sports')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas, pred_class = predict(modelV1_loaded, ex_text_str, text_pipeline)\n",
    "probas, pred_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63e79917a05e390872358bfb73c58bc903ada01d2d04077091749088207d82cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
